{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78415237-4781-4a0b-bc1c-1220940aff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4715780-0375-42c2-bab0-f4dd467da3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   -1\n",
       "1    2\n",
       "2    3\n",
       "3    4\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'EmpCode': ['E1', 'E2', 'E3', 'E4', 'E5'],\n",
    "    'Gender': ['Male', 'Female', 'Female', 'Male', 'Male'],\n",
    "    'Age': [27, 24, 29, 24, 25],\n",
    "    'Department': ['Accounting', 'Sales', 'Accounting', np.nan, 'Sales'],\n",
    "    'Class': [2,3,4,-1,-1]\n",
    "\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "label_col = \"Class\"\n",
    "all_classes = pd.DataFrame(df[label_col].value_counts().keys()).iloc[:, 0]\n",
    "all_classes\n",
    "# all_classes, {k: v for v, k in enumerate(all_classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ef3fab9-cccf-4a15-9f0d-08b070225a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[False, False, False, False],\n",
       " [False, False, False, False],\n",
       " [False, False, False, False]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visited = [ [ False for _ in range(4)] for _ in range(3) ]\n",
    "visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8a8f72fe-e260-4ed0-ac29-1645c402ece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HierarchicalSoftmax(nn.Module):\n",
    "\n",
    "    def __init__(self, ntokens, nhid, ntokens_per_class=None, **kwargs):\n",
    "        super(HierarchicalSoftmax, self).__init__()\n",
    "\n",
    "        # Parameters\n",
    "        self.ntokens = ntokens\n",
    "        self.nhid = nhid\n",
    "\n",
    "        if ntokens_per_class is None:\n",
    "            ntokens_per_class = int(np.ceil(np.sqrt(ntokens)))\n",
    "\n",
    "        self.ntokens_per_class = ntokens_per_class\n",
    "\n",
    "        self.nclasses = int(np.ceil(self.ntokens * 1. / self.ntokens_per_class))\n",
    "        self.ntokens_actual = self.nclasses * self.ntokens_per_class\n",
    "\n",
    "        self.layer_top_W = nn.Parameter(torch.FloatTensor(self.nhid, self.nclasses), requires_grad=True)\n",
    "        # print(self.layer_top_W.shape)\n",
    "        self.layer_top_b = nn.Parameter(torch.FloatTensor(self.nclasses), requires_grad=True)\n",
    "\n",
    "        self.layer_bottom_W = nn.Parameter(torch.FloatTensor(self.nclasses, self.nhid, self.ntokens_per_class),\n",
    "                                           requires_grad=True)\n",
    "        # print(self.layer_bottom_W.shape)\n",
    "        self.layer_bottom_b = nn.Parameter(torch.FloatTensor(self.nclasses, self.ntokens_per_class), requires_grad=True)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.layer_top_W.data.uniform_(-initrange, initrange)\n",
    "        self.layer_top_b.data.fill_(0)\n",
    "        self.layer_bottom_W.data.uniform_(-initrange, initrange)\n",
    "        self.layer_bottom_b.data.fill_(0)\n",
    "        \n",
    "    def _predict(self, inputs):\n",
    "        batch_size, d = inputs.size()\n",
    "\n",
    "        layer_top_logits = torch.matmul(inputs, self.layer_top_W) + self.layer_top_b\n",
    "        layer_top_probs = self.softmax(layer_top_logits)\n",
    "        \n",
    "        label_position_top = torch.argmax(layer_top_probs, dim=1)\n",
    "                \n",
    "        layer_bottom_logits = torch.squeeze(\n",
    "            torch.bmm(torch.unsqueeze(inputs, dim=1), self.layer_bottom_W[label_position_top]), dim=1) + \\\n",
    "                              self.layer_bottom_b[label_position_top]\n",
    "        layer_bottom_probs = self.softmax(layer_bottom_logits)\n",
    "        \n",
    "        return torch.bmm(layer_top_probs.unsqueeze(2), layer_bottom_probs.unsqueeze(1)).flatten(start_dim=1)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs, labels=None):\n",
    "        if labels is None:\n",
    "            return self._predict(inputs)\n",
    "        batch_size, d = inputs.size()\n",
    "\n",
    "        layer_top_logits = torch.matmul(inputs, self.layer_top_W) + self.layer_top_b\n",
    "        layer_top_probs = self.softmax(layer_top_logits)\n",
    "        \n",
    "        label_position_top = (labels / self.ntokens_per_class).long()\n",
    "        label_position_bottom = (labels % self.ntokens_per_class).long()\n",
    "        \n",
    "        # print(layer_top_probs.shape, label_position_top.shape)\n",
    "\n",
    "        layer_bottom_logits = torch.squeeze(\n",
    "            torch.bmm(torch.unsqueeze(inputs, dim=1), self.layer_bottom_W[label_position_top]), dim=1) + \\\n",
    "                              self.layer_bottom_b[label_position_top]\n",
    "        layer_bottom_probs = self.softmax(layer_bottom_logits)\n",
    "\n",
    "        target_probs = layer_top_probs[torch.arange(batch_size).long(), label_position_top] * layer_bottom_probs[\n",
    "            torch.arange(batch_size).long(), label_position_bottom]\n",
    "\n",
    "        # print(f\"top {layer_top_probs.shape} {layer_top_probs}\")\n",
    "        # print(f\"bottom {layer_bottom_probs.shape} {layer_bottom_probs}\")\n",
    "        top_indx = torch.argmax(layer_top_probs, dim=1)\n",
    "        botton_indx = torch.argmax(layer_bottom_probs, dim=1)\n",
    "\n",
    "        real_indx = (top_indx * self.ntokens_per_class) + botton_indx\n",
    "        # print(top_indx, self.nclasses, botton_indx)\n",
    "        # print(f\"target {target_probs.shape} {target_probs}\")\n",
    "\n",
    "        # loss = -torch.mean(torch.log(target_probs.type(torch.float32) + 1e-3))\n",
    "        loss = -torch.mean(torch.log(target_probs))\n",
    "        with torch.no_grad():\n",
    "            preds = torch.bmm(layer_top_probs.unsqueeze(2), layer_bottom_probs.unsqueeze(1)).flatten(start_dim=1)\n",
    "\n",
    "        return loss, target_probs, layer_top_probs, layer_bottom_probs, top_indx, botton_indx, real_indx, preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "20cb4a62-61a5-44f6-b601-eda08081e593",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObservationTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder : nn.Module, nhead=10, feature_dim = 1000, output_size=128):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.head = nn.TransformerEncoderLayer(d_model=feature_dim, nhead=nhead, batch_first=True)\n",
    "        self.decoder = nn.Linear(feature_dim, output_size)\n",
    "        self.feature_dim = feature_dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, S, C, R, _ = x.shape \n",
    "        x = x.reshape(-1, C, R, R) # squeeze observations to (batch * sequence, channel, witdh, height) \n",
    "                                    # size into a single dim for encoding as batch\n",
    "        features = self.encoder(x) # encode all images for B observations of S images each\n",
    "        features = features.reshape(B, S, self.feature_dim) # resize back to observation sizes (batch, sequence, )\n",
    "        out = head(features) #apply attention to teh features\n",
    "        return self.decoder(out.mean(1))\n",
    "\n",
    "    \n",
    "class HObservationTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder : nn.Module, nhead=10, feature_dim = 1000, classes=80000, ntokens_per_class=80):\n",
    "        super().__init__()\n",
    "        self.obs_transformer = ObservationTransformer(encoder=encoder, nhead=nhead, feature_dim=feature_dim, output_size=128)\n",
    "        self.hs =  HierarchicalSoftmax(ntokens=classes, nhid=128, ntokens_per_class=ntokens_per_class)\n",
    "        # print(self.hs.nclasses, self.hs.ntokens_per_class)\n",
    "        \n",
    "    def forward(self, x, y=None):\n",
    "        assert len(x.shape) == 5, \"Observation shape should be (batch, sequence, channels, witdh, height)\"\n",
    "        x = self.obs_transformer(x) \n",
    "        return self.hs(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a9cac5da-8b13-44fc-8048-6f15ccfde894",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = models.resnet50(pretrained=True)\n",
    "m = ObservationTransformer(encoder=encoder, nhead=10, feature_dim=1000, output_size=128)\n",
    "hm = HObservationTransformer(encoder=encoder, nhead=10, feature_dim=1000, classes=80000, ntokens_per_class=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "8790a0ca-82b9-407e-9093-3e112ca55499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2, 4, 3, 224, 224)\n",
    "y = m(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "150f06cd-d29d-4ddd-bdfb-ef2a1ded31d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 80000])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2, 4, 3, 224, 224)\n",
    "labels = torch.ones(2).long()\n",
    "loss, target_probs, layer_top_probs, layer_bottom_probs, top_indx, botton_indx, real_indx, preds = hm(x, labels)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a5626861-f170-47dd-bd6c-22f6160b764f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 80000])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2, 4, 3, 224, 224)\n",
    "preds = hm(x)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ed69a50-6125-48ed-9211-a7c5df0497ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1000])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, S, D = features.shape\n",
    "out = head(features)\n",
    "out = decoder(out.mean(0))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c749f5af-7a5d-4028-85eb-8353d126672a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6]),\n",
       " tensor([[  10,   20,   30,   20,   40,   60],\n",
       "         [ 400,  800, 1200,  500, 1000, 1500]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_top_probs = torch.tensor([[1, 2], [4, 5]]) \n",
    "layer_bottom_probs = torch.tensor([[10, 20, 30], [100, 200, 300]])\n",
    "\n",
    "# layer_top_probs = torch.rand(2, 1000)\n",
    "# layer_bottom_probs = torch.rand(2, 80)\n",
    "\n",
    "preds = torch.bmm(layer_top_probs.unsqueeze(2), layer_bottom_probs.unsqueeze(1)).flatten(start_dim=1)\n",
    "preds.shape, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf68849d-0013-4cd6-be5a-45812c23501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def get_mrr(predictions_dict):\n",
    "    ranks = np.asarray([get_rank(value) for key, value in predictions_dict.items()])\n",
    "    return np.sum((1 / ranks)) / len(predictions_dict)\n",
    "\n",
    "def get_rank(dict_value):\n",
    "    prob = dict_value['prob']\n",
    "    label = dict_value['label']\n",
    "    idx = np.argsort(prob)[::-1]\n",
    "    np.argmax(prob) == label\n",
    "    rank_i = np.squeeze(np.where(idx == label)) + 1\n",
    "\n",
    "    return rank_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a943826-5d08-44e2-9888-40da517e6bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "# class ViT(nn.Module):\n",
    "#     def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "#         super().__init__()\n",
    "#         image_height, image_width = pair(image_size)\n",
    "#         patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "#         assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "#         num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "#         patch_dim = channels * patch_height * patch_width\n",
    "#         assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "#         self.to_patch_embedding = nn.Sequential(\n",
    "#             Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "#             nn.Linear(patch_dim, dim),\n",
    "#         )\n",
    "\n",
    "#         self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "#         self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "#         self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "#         self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "#         self.pool = pool\n",
    "#         self.to_latent = nn.Identity()\n",
    "\n",
    "#         self.mlp_head = nn.Sequential(\n",
    "#             nn.LayerNorm(dim),\n",
    "#             nn.Linear(dim, num_classes)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, img):\n",
    "#         x = self.to_patch_embedding(img)\n",
    "#         b, n, _ = x.shape\n",
    "\n",
    "#         cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "#         x = torch.cat((cls_tokens, x), dim=1)\n",
    "#         x += self.pos_embedding[:, :(n + 1)]\n",
    "#         x = self.dropout(x)\n",
    "\n",
    "#         x = self.transformer(x)\n",
    "\n",
    "#         x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "#         x = self.to_latent(x)\n",
    "#         return self.mlp_head(x)\n",
    "    \n",
    "    \n",
    "class ViTEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, *, image_size, patch_size, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0., flatten=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.dim = dim\n",
    "        self.heads = heads\n",
    "        self.dim_head = dim_head\n",
    "        self.depth = depth\n",
    "        self.flatten = flatten\n",
    "        \n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "        if self.flatten:\n",
    "            return torch.flatten(x, start_dim=1)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class ModularViT(nn.Module):\n",
    "    def __init__(self, *, encoder, num_classes, mlp_dim, pool = 'cls', channels = 3, dropout = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.encoder.flatten = False\n",
    "        self.decoder = Transformer(self.encoder.dim, self.encoder.depth, self.encoder.heads, self.encoder.dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(self.encoder.dim),\n",
    "            nn.Linear(self.encoder.dim, num_classes)\n",
    "        )\n",
    "        \n",
    "    def freeze_encoder(self):\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    \n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.encoder(img)\n",
    "        \n",
    "        x = self.decoder(x)\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "        x = self.to_latent(x)\n",
    "        return self.mlp_head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f692c401-d17e-46e3-a3ba-16f5f5e4790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = ViTEncoder(\n",
    "    image_size = 224,\n",
    "    patch_size = 32,\n",
    "    dim = 128,\n",
    "    depth = 3,\n",
    "    heads = 16,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1,\n",
    "    mlp_dim = 2048,\n",
    "    flatten = True\n",
    ")\n",
    "# model = ModularViT(encoder=enc, num_classes=80000, pool = 'cls', channels = 3, dropout = 0.1, mlp_dim=2048)\n",
    "# model.freeze_encoder()\n",
    "\n",
    "img = torch.randn(4, 3, 224, 224)\n",
    "\n",
    "enc_preds = enc(img)\n",
    "# preds = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60aff200-df1d-4c57-8633-4212493ff002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 80000]), torch.Size([4, 6400]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape, enc_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b1573547-90a6-4b3d-8f00-175a3ca69a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.rand((4, 10, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0cd472c-7fcf-46a6-8d1c-592e797e85fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 32])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "042dffae-5cb7-46e2-8ef3-552a4bb2d5ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 32])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[:, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b71698-a066-4884-aa46-465d566e042d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
